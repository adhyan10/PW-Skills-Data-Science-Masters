{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2a8952",
   "metadata": {},
   "source": [
    "## Web Scraping Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78961b05",
   "metadata": {},
   "source": [
    "1.  What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e8a89",
   "metadata": {},
   "source": [
    "Ans: **Web scraping is a process where we write a program which extraxts data/information from a website on internet.**  \n",
    "**The use of web scraping is very eminent in field of data science.Lots of important data is stored on the interent which can be needed for the process of data scientific analysis and analytics,which can be acquired through web scraping.**  \n",
    "**Following are the areas where web scaping is used:**  \n",
    "1. **Lead generation for marketing**\n",
    "2. **Price comparison and Competition**\n",
    "3. **E-commerce**\n",
    "4. **Real estate.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaddd3c3",
   "metadata": {},
   "source": [
    "2.  What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d93f0b4",
   "metadata": {},
   "source": [
    "Ans: **Following are the ways/methods of scraping:**   \n",
    "1. **Copy and pasting the content.This is not recommended.**\n",
    "2. **Writing code using python and its various libraries like beautifulsoup,selenium.**\n",
    "3. **HTML parsing is done with JavaScript and targets linear or nested HTML pages.**\n",
    "4. **DOM parsing: DOM is short for Document Object Model and it defines the style structure and content of XML files. Scrapers make use of DOM parsers to get an in-depth view of a web page’s structure.**\n",
    "5. **Proprietary softwares are avialable in market for scraping.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ea3ef",
   "metadata": {},
   "source": [
    "3.  What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2534699",
   "metadata": {},
   "source": [
    "Ans : **Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML,which is useful for web scraping.It is used for scraping web pages of a desired website.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f16430",
   "metadata": {},
   "source": [
    "4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b56f34b",
   "metadata": {},
   "source": [
    "Ans: **In this project of web scraping we are scaping the comments and review ratings of a specified product in flipkart website that we are searching .First we want to search the product and then we want to check its reviews and ratings .While the scraping \n",
    "part is done by libraries like requests and beautifulsoup , we needed to create a web app to get user input for product name and a page with results showing the reviews and rating.This is done using the flask library.Flask is a pyhton based web development/application framework,therefore we use it in our project as it is easy to learn and understand when compared with other python web application frameworks  like django.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50129906",
   "metadata": {},
   "source": [
    "5.  Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe46d62",
   "metadata": {},
   "source": [
    "Ans: **The 2  aws services that we use in this project are:** \n",
    "1. **Elastic Beanstalk : Elastic Beanstalk is a service for deploying and scaling web applications and services. Upload your code and Elastic Beanstalk automatically handles the deployment—from capacity provisioning, load balancing, and auto scaling to application health monitoring.It provides us with resources like ram, memory and CPU to create an environment on which our application is deployed.In our project we create a python environment for our scraping application.**  \n",
    "\n",
    "2. **Code Pipeline : AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.In the project code pipeline takes code from a github repository which has our flask application in it and provide that code to the environment which we later deploy using the elastic beanstalk service that we created.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1591d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
