{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48fea84a-72b4-46fb-bc86-a4d34568aa7d",
   "metadata": {},
   "source": [
    "# Ensemble Techniques And Its Types-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257dd5e-7dce-4a3d-b5c9-ca7b85363947",
   "metadata": {},
   "source": [
    "**Q1. What is an ensemble technique in machine learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79354a1-a3a9-4917-ad63-d8995c930acf",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "\n",
    "**Ensemble Techniques in Machine Learning**\n",
    "\n",
    "An ensemble technique in machine learning involves combining the predictions of multiple models to improve overall performance. The idea is that by aggregating the outputs of several models, you can achieve better accuracy, robustness, and generalization compared to using a single model.\n",
    "\n",
    "**Types of Ensemble Methods**\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**: This technique involves training multiple models on different subsets of the training data, which are created by sampling with replacement. The final prediction is made by aggregating the predictions of all individual models, often by averaging (for regression) or voting (for classification). A popular example is the Random Forest algorithm.\n",
    "\n",
    "2. **Boosting**: In boosting, models are trained sequentially, with each model focusing on the errors made by the previous ones. The idea is to correct the mistakes of earlier models by giving more weight to the misclassified instances. The final prediction is a weighted combination of the predictions from all models. Examples include AdaBoost and Gradient Boosting Machines (GBM).\n",
    "\n",
    "3. **Stacking (Stacked Generalization)**: This approach involves training multiple base models and then combining their predictions using another model called a meta-learner. The base models are typically trained on the original data, and the meta-learner is trained to make the final prediction based on the outputs of the base models.\n",
    "\n",
    "Ensemble techniques can help in mitigating issues such as overfitting and high variance, and they often lead to more robust and reliable predictions compared to single models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf35fe-1c77-4201-939f-4f7b920c928b",
   "metadata": {},
   "source": [
    "**Q2. Why are ensemble techniques used in machine learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aebe2d-762a-40c0-b4d9-cde9dfa567ba",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "\n",
    "**Why Ensemble Techniques Are Used in Machine Learning**\n",
    "\n",
    "**1. Improved Accuracy**: By combining multiple models, ensemble methods often achieve higher accuracy than any individual model. This is because different models may capture different aspects of the data, and their combined predictions can lead to better performance.\n",
    "\n",
    "**2. Reduced Overfitting**: Ensembles can reduce the risk of overfitting, especially when using complex models. By averaging or voting among several models, ensembles smooth out the predictions and generalize better to new, unseen data.\n",
    "\n",
    "**3. Enhanced Robustness**: Ensembles are generally more robust to noisy data and outliers. If one model makes errors due to noise or outliers, other models in the ensemble may not be affected as much, leading to more stable predictions.\n",
    "\n",
    "**4. Better Generalization**: By combining models that learn different aspects of the data, ensembles can generalize better to new data. This is particularly useful in scenarios where no single model captures all relevant patterns in the data.\n",
    "\n",
    "**5. Mitigation of Bias and Variance**: Different ensemble techniques address bias and variance in different ways:\n",
    "   - **Bagging** (Bootstrap Aggregating) primarily helps in reducing variance.\n",
    "   - **Boosting** helps in reducing both bias and variance by focusing on correcting errors made by previous models.\n",
    "   - **Stacking** combines predictions from diverse models to leverage their individual strengths, balancing bias and variance.\n",
    "\n",
    "**6. Flexibility**: Ensembles can combine various types of models (e.g., decision trees, neural networks, etc.), allowing for greater flexibility and the ability to leverage the strengths of different algorithms.\n",
    "\n",
    "**7. Handling Imbalanced Data**: Techniques like boosting can be particularly effective in handling imbalanced datasets by giving more weight to underrepresented classes or examples, thus improving the performance on minority classes.\n",
    "\n",
    "**8. Avoiding Over-reliance on a Single Model**: Relying on a single model can be risky if that model is not well-tuned or if it has specific weaknesses. Ensembles reduce this risk by incorporating multiple perspectives on the data.\n",
    "\n",
    "Overall, ensemble techniques provide a way to leverage the strengths of multiple models, leading to more accurate, reliable, and generalizable machine learning systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ab1872-4349-4aba-b9e6-bca98495e390",
   "metadata": {},
   "source": [
    "**Q3. What is bagging?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940b362-1c43-447b-9937-0b322991a38c",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "\n",
    "**Bagging (Bootstrap Aggregating)** is an ensemble technique in machine learning designed to improve the performance and robustness of predictive models. The core idea of bagging is to reduce variance and enhance the stability of machine learning algorithms by combining the predictions of multiple models trained on different subsets of the data.\n",
    "\n",
    "**How Bagging Works**\n",
    "\n",
    "**1. Data Sampling**: Bagging involves creating multiple subsets of the training data through a process called bootstrapping. Each subset is generated by randomly sampling the original training data with replacement. This means that each subset can contain duplicate instances and may miss some instances from the original dataset.\n",
    "\n",
    "**2. Model Training**: A separate model is trained on each of these bootstrapped subsets. Since the subsets differ, each model learns slightly different patterns from the data.\n",
    "\n",
    "**3. Aggregation**: After all models are trained, their predictions are aggregated to make a final prediction. For regression tasks, this is typically done by averaging the predictions of all models. For classification tasks, the predictions are often combined using majority voting, where the class that receives the most votes from the individual models is selected as the final prediction.\n",
    "\n",
    "**4. Prediction**: When making predictions on new data, each model in the ensemble provides its prediction. These individual predictions are then aggregated using the same method as during training to produce the final prediction.\n",
    "\n",
    "**Key Benefits of Bagging**\n",
    "\n",
    "- **Reduces Variance**: By averaging the predictions of multiple models, bagging reduces the overall variance of the predictions, leading to more stable and reliable results. This is particularly useful when the base models are prone to overfitting.\n",
    "\n",
    "- **Improves Performance**: Bagging can improve the performance of weak learners (models with high variance) by reducing their sensitivity to the fluctuations in the training data.\n",
    "\n",
    "- **Enhances Robustness**: Bagging makes the ensemble more robust to noisy data and outliers since the influence of any single noisy data point is diluted across multiple models.\n",
    "\n",
    "**Popular Example**: A well-known example of bagging is the **Random Forest** algorithm. In Random Forests, multiple decision trees are trained on different bootstrapped subsets of the data, and their predictions are aggregated to produce the final result.\n",
    "\n",
    "Bagging is a powerful technique for improving model performance and stability, especially when used with algorithms that have high variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101b385-7510-499f-896a-4c7894f53aaa",
   "metadata": {},
   "source": [
    "**Q4. What is boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131749fd-d104-4312-84ae-df2e9dbddb84",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "\n",
    "**Boosting** is an ensemble learning technique designed to improve the performance of machine learning models by combining multiple weak learners to create a strong predictive model. Unlike bagging, which focuses on reducing variance, boosting primarily aims to reduce bias and improve model accuracy by focusing on the errors made by previous models.\n",
    "\n",
    "**How Boosting Works**\n",
    "\n",
    "**1. Sequential Model Training**: Boosting involves training models sequentially, where each model is trained to correct the errors made by the previous models. This sequential approach allows each new model to focus on the data points that previous models have misclassified or predicted poorly.\n",
    "\n",
    "**2. Weight Adjustment**: In each iteration of boosting, the algorithm adjusts the weights of the training instances based on the errors of the previous model. Misclassified instances are given more weight, so the new model will pay more attention to these harder cases. Correctly classified instances are given less weight.\n",
    "\n",
    "**3. Model Combination**: After each model is trained, its predictions are combined with those of previous models. The combination is typically done by weighting the models' predictions based on their performance, with more accurate models receiving higher weights. The final prediction is made by aggregating these weighted predictions.\n",
    "\n",
    "**4. Prediction**: For new data, the final prediction is made by combining the predictions from all the models in the ensemble, using the same weighting scheme applied during training.\n",
    "\n",
    "**Key Benefits of Boosting**\n",
    "\n",
    "- **Reduces Bias**: Boosting helps in reducing bias by focusing on the errors of previous models and iteratively correcting them. This can lead to a significant improvement in accuracy.\n",
    "\n",
    "- **Improves Performance**: Boosting often results in a strong predictive model that outperforms individual weak learners. It effectively combines the strengths of multiple models to create a more accurate final model.\n",
    "\n",
    "- **Handles Complex Data**: Boosting is capable of capturing complex patterns in the data that may be missed by individual models, making it suitable for a wide range of problems.\n",
    "\n",
    "- **Robust to Overfitting**: Although boosting can be prone to overfitting if not properly tuned, it generally performs well with appropriate regularization techniques, especially on smaller datasets.\n",
    "\n",
    "**Popular Examples of Boosting Algorithms**\n",
    "\n",
    "- **AdaBoost (Adaptive Boosting)**: AdaBoost adjusts the weights of training instances based on the errors of previous models and combines the models' predictions through a weighted vote.\n",
    "\n",
    "- **Gradient Boosting Machines (GBM)**: GBM builds models sequentially, each one correcting the errors of the previous model, and optimizes a loss function by minimizing the residual errors.\n",
    "\n",
    "- **XGBoost (Extreme Gradient Boosting)**: XGBoost is an optimized implementation of gradient boosting that is known for its efficiency and performance, particularly in large-scale machine learning tasks.\n",
    "\n",
    "- **LightGBM**: LightGBM is another efficient gradient boosting framework designed to handle large datasets and high-dimensional data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ba39e1-7527-4171-8005-631fe78b9ffd",
   "metadata": {},
   "source": [
    "**Q5. What are the benefits of using ensemble techniques?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7437e0-d72b-4358-b2a7-6c5e02a7064c",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "\n",
    "**Benefits of Using Ensemble Techniques:**\n",
    "\n",
    "Ensemble techniques in machine learning involve combining multiple models to create a more powerful predictive system. Here are some of the key benefits of using ensemble techniques:\n",
    "\n",
    "**1. Improved Accuracy**  \n",
    "Ensemble methods often achieve higher accuracy compared to individual models. By aggregating predictions from multiple models, ensembles can capture more complex patterns and make more accurate predictions.\n",
    "\n",
    "**2. Reduced Overfitting**  \n",
    "Ensemble techniques can help reduce overfitting, particularly when the individual models have high variance. Combining multiple models can smooth out predictions and make the ensemble more generalized, reducing the risk of overfitting to the training data.\n",
    "\n",
    "**3. Enhanced Robustness**  \n",
    "Ensemble methods are more robust to noisy data and outliers. If individual models are affected by noise or outliers, the effect can be mitigated when their predictions are aggregated, leading to more stable results.\n",
    "\n",
    "**4. Better Generalization**  \n",
    "By leveraging multiple models, ensembles can generalize better to unseen data. Different models may capture different aspects of the data, and their combined predictions can provide a more comprehensive understanding of the data.\n",
    "\n",
    "**5. Mitigation of Bias and Variance**  \n",
    "Different ensemble techniques address bias and variance in different ways:\n",
    "   - **Bagging** (Bootstrap Aggregating) primarily reduces variance.\n",
    "   - **Boosting** helps in reducing both bias and variance by focusing on correcting the errors of previous models.\n",
    "   - **Stacking** (Stacked Generalization) balances bias and variance by combining diverse models through a meta-learner.\n",
    "\n",
    "**6. Flexibility**  \n",
    "Ensemble methods can combine various types of models (e.g., decision trees, neural networks, etc.), allowing for greater flexibility and the ability to leverage the strengths of different algorithms.\n",
    "\n",
    "**7. Handling Imbalanced Data**  \n",
    "Ensemble techniques, particularly boosting methods, can be effective in handling imbalanced datasets. They often provide mechanisms to give more weight to underrepresented classes or instances, improving performance on minority classes.\n",
    "\n",
    "**8. Avoiding Over-reliance on a Single Model**  \n",
    "Relying on a single model can be risky if that model has specific weaknesses or is not well-tuned. Ensembles reduce this risk by incorporating multiple models, each of which may address different aspects of the data.\n",
    "\n",
    "**9. Improved Model Stability**  \n",
    "Ensemble methods can make models more stable by reducing the sensitivity of predictions to variations in the training data. This can lead to more reliable and consistent predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f79340-e125-4547-96dd-daeeb66e8b24",
   "metadata": {},
   "source": [
    "**Q6. Are ensemble techniques always better than individual models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac4ea13-2190-4522-80d3-23358239abe9",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "  \n",
    "Ensemble techniques are not always better than individual models. While they offer many advantages, there are specific situations where using individual models might be more appropriate. Here are some considerations to keep in mind:\n",
    "\n",
    "**Advantages of Ensemble Techniques**\n",
    "\n",
    "1. **Improved Performance**: Ensembles often achieve better performance than individual models by combining multiple predictions, which can capture a broader range of patterns and reduce errors.\n",
    "\n",
    "2. **Reduced Variance and Overfitting**: By averaging predictions or using majority voting, ensembles can reduce the variance of the predictions and mitigate the risk of overfitting, especially when dealing with complex models.\n",
    "\n",
    "3. **Robustness**: Ensembles can be more robust to noise and outliers since the effect of noisy data points is diluted across multiple models.\n",
    "\n",
    "**Situations Where Individual Models Might Be Preferable**\n",
    "\n",
    "1. **Increased Complexity**: Ensembles introduce additional complexity in terms of training and maintaining multiple models. For some applications, especially those with resource constraints, a simpler individual model might be preferable.\n",
    "\n",
    "2. **Computational Cost**: Training and evaluating multiple models can be computationally expensive and time-consuming. If computational resources are limited, using a single, well-tuned model might be more practical.\n",
    "\n",
    "3. **Interpretability**: Individual models, especially simpler ones like decision trees or linear models, can be more interpretable compared to ensembles. If model interpretability is crucial, a single model might be preferred.\n",
    "\n",
    "4. **Diminishing Returns**: In some cases, the performance improvement gained from using ensembles may be marginal. If the individual model already performs exceptionally well, the additional complexity of an ensemble might not justify the small gains in performance.\n",
    "\n",
    "5. **Data Availability**: For very small datasets, individual models may perform better due to the risk of overfitting with complex ensemble methods. In such cases, simpler models might be more suitable.\n",
    "\n",
    "6. **Specific Use Cases**: Certain problems might not benefit significantly from ensemble techniques. For example, if a problem is straightforward and well-suited to a specific model type, the added complexity of an ensemble might not provide substantial benefits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e3de5c-5c54-4d1e-a534-f54e662a42c3",
   "metadata": {},
   "source": [
    "**Q7. How is the confidence interval calculated using bootstrap?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c981c58-04d5-4e31-aa40-1bcdec2dd757",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "  \n",
    "The bootstrap method is a resampling technique used to estimate the distribution of a statistic and calculate confidence intervals. Here's a step-by-step guide on how to calculate confidence intervals using bootstrap:\n",
    "\n",
    "#### **Steps to Calculate Confidence Interval Using Bootstrap**\n",
    "\n",
    "1. **Original Sample**: Start with your original dataset, which contains `n` observations.\n",
    "\n",
    "2. **Resampling**: Perform resampling with replacement to generate multiple bootstrap samples from the original dataset. Each bootstrap sample is of the same size `n` as the original dataset but may include duplicate observations and may omit some observations from the original dataset.\n",
    "\n",
    "3. **Compute Statistic**: For each bootstrap sample, compute the statistic of interest (e.g., mean, median, standard deviation, etc.). This will give you a distribution of the statistic based on the resampled data.\n",
    "\n",
    "4. **Repeat**: Repeat the resampling and statistic computation process a large number of times (e.g., 1,000 or 10,000 times) to build a distribution of the bootstrap estimates for the statistic.\n",
    "\n",
    "5. **Calculate Confidence Interval**: Determine the confidence interval from the distribution of bootstrap estimates. The most common method is to use the percentile method, but other methods such as the bias-corrected and accelerated (BCa) method are also used.\n",
    "\n",
    "   - **Percentile Method**:\n",
    "     1. Sort the bootstrap estimates in ascending order.\n",
    "     2. Determine the desired percentiles based on the confidence level. For example, for a 95% confidence interval, you would find the 2.5th percentile and the 97.5th percentile of the sorted bootstrap estimates.\n",
    "     3. The values at these percentiles represent the lower and upper bounds of the confidence interval.\n",
    "\n",
    "   - **Bias-Corrected and Accelerated (BCa) Method** (optional):\n",
    "     1. Compute the bootstrap estimate's bias and acceleration factor.\n",
    "     2. Adjust the percentile-based confidence interval using these factors to correct for bias and skewness.\n",
    "\n",
    "#### **Example of Bootstrap Confidence Interval Calculation**\n",
    "\n",
    "Here's an example of how you might calculate a 95% confidence interval for the mean of a dataset using the percentile method:\n",
    "\n",
    "1. **Original Data**: Suppose you have a dataset with 100 observations.\n",
    "\n",
    "2. **Resampling**: Generate 1,000 bootstrap samples by sampling with replacement from the original dataset.\n",
    "\n",
    "3. **Compute Mean**: For each bootstrap sample, calculate the mean. This gives you 1,000 bootstrap mean estimates.\n",
    "\n",
    "4. **Sort Estimates**: Sort the 1,000 bootstrap mean estimates.\n",
    "\n",
    "5. **Determine Percentiles**:\n",
    "   - For a 95% confidence interval, find the 2.5th percentile and the 97.5th percentile of the sorted bootstrap means.\n",
    "   - Suppose these percentiles are 50.2 and 54.6, respectively.\n",
    "\n",
    "6. **Confidence Interval**: The 95% confidence interval for the mean is [50.2, 54.6].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c6fa7-78ba-4f88-bb58-f7996f7c8c5c",
   "metadata": {},
   "source": [
    "**Q8. How does bootstrap work and What are the steps involved in bootstrap?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69168d-1e92-4c86-9631-fec54f2af442",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "  \n",
    "Bootstrap is a resampling technique used to estimate the distribution of a statistic from a sample data set. It involves generating multiple simulated samples (bootstrap samples) from the original data set to assess the variability and statistical properties of an estimator. Hereâ€™s how it works and the steps involved:\n",
    "\n",
    "#### **How Bootstrap Works**\n",
    "\n",
    "1. **Original Sample**: Start with your original dataset, which contains `n` observations.\n",
    "\n",
    "2. **Resampling with Replacement**: Create multiple bootstrap samples by repeatedly sampling from the original dataset with replacement. Each bootstrap sample will have the same size `n` as the original dataset but will differ in terms of the specific observations included.\n",
    "\n",
    "3. **Calculate Statistic**: For each bootstrap sample, compute the statistic of interest (e.g., mean, median, variance). This gives a collection of statistic estimates that reflect the sampling distribution of the statistic.\n",
    "\n",
    "4. **Estimate Distribution**: Analyze the distribution of the computed statistics from all bootstrap samples. This distribution can be used to estimate various statistical properties, such as the standard error or confidence intervals.\n",
    "\n",
    "5. **Inferential Statistics**: Use the distribution of bootstrap estimates to make inferences about the original dataset. For example, you can calculate confidence intervals or perform hypothesis tests based on the bootstrap distribution.\n",
    "\n",
    "#### **Steps Involved in Bootstrap**\n",
    "\n",
    "1. **Obtain Original Data**: Start with your original dataset, which consists of `n` data points.\n",
    "\n",
    "2. **Generate Bootstrap Samples**:\n",
    "   - **Sampling with Replacement**: Randomly draw `n` observations from the original dataset with replacement to create a bootstrap sample. Each data point can be selected more than once, and some data points may not be selected at all.\n",
    "   - **Repeat**: Repeat this resampling process many times (e.g., 1,000 or 10,000 times) to create a large number of bootstrap samples.\n",
    "\n",
    "3. **Compute Statistic for Each Bootstrap Sample**:\n",
    "   - For each bootstrap sample, compute the statistic of interest (e.g., mean, median, variance). This results in a distribution of the statistic across all bootstrap samples.\n",
    "\n",
    "4. **Analyze the Bootstrap Distribution**:\n",
    "   - **Compute Summary Statistics**: Calculate summary statistics such as the mean, standard deviation, and percentiles of the bootstrap distribution.\n",
    "   - **Construct Confidence Intervals**: Use methods such as the percentile method or bias-corrected and accelerated (BCa) method to construct confidence intervals based on the bootstrap distribution.\n",
    "\n",
    "5. **Make Inferences**:\n",
    "   - **Estimate Variability**: Use the bootstrap distribution to estimate the variability of the statistic and to assess its accuracy.\n",
    "   - **Hypothesis Testing**: Perform hypothesis tests by comparing the bootstrap distribution to theoretical distributions or by evaluating the proportion of bootstrap samples that meet specific criteria.\n",
    "\n",
    "#### **Example of Bootstrap Procedure**\n",
    "\n",
    "1. **Original Data**: Suppose you have a dataset with 100 observations.\n",
    "\n",
    "2. **Generate Bootstrap Samples**: Create 1,000 bootstrap samples, each containing 100 observations drawn with replacement from the original dataset.\n",
    "\n",
    "3. **Compute Statistic**: For each bootstrap sample, calculate the mean.\n",
    "\n",
    "4. **Analyze Distribution**:\n",
    "   - **Sort the Means**: Sort the 1,000 bootstrap means.\n",
    "   - **Calculate Confidence Intervals**: Find the 2.5th percentile and the 97.5th percentile of the sorted means to get a 95% confidence interval.\n",
    "\n",
    "5. **Inference**: The confidence interval from the bootstrap distribution provides an estimate of the range within which the true population mean is likely to fall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82658419-1d20-4a6a-951d-5097bfb97af1",
   "metadata": {},
   "source": [
    "**Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bea036-931c-467b-adbd-18cc135db066",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "\n",
    "**Estimating the 95% Confidence Interval for Population Mean Height Using Bootstrap**\n",
    "\n",
    "To estimate the 95% confidence interval for the population mean height using the bootstrap method, follow these steps:\n",
    "\n",
    "#### **Bootstrap Procedure**\n",
    "\n",
    "1. **Original Data**:\n",
    "   - **Sample size**: `n = 50`\n",
    "   - **Sample mean height**: `15 meters`\n",
    "   - **Sample standard deviation**: `2 meters`\n",
    "   - **Original sample data**: Assume the heights are approximately normally distributed, or simulate the data if not provided.\n",
    "\n",
    "2. **Generate Bootstrap Samples**:\n",
    "   - Draw bootstrap samples from the original data with replacement.\n",
    "   - Each bootstrap sample should have the same size as the original sample, i.e., 50 observations.\n",
    "   - Repeat this process many times (e.g., 1,000 or 10,000 times) to generate a large number of bootstrap samples.\n",
    "\n",
    "3. **Compute Statistic**:\n",
    "   - For each bootstrap sample, compute the mean height.\n",
    "\n",
    "4. **Analyze the Bootstrap Distribution**:\n",
    "   - Collect the means from all bootstrap samples to form the bootstrap distribution of the mean height.\n",
    "   - Sort the bootstrap mean heights in ascending order.\n",
    "\n",
    "5. **Determine Confidence Interval**:\n",
    "   - For a 95% confidence interval, find the 2.5th percentile and the 97.5th percentile of the sorted bootstrap means.\n",
    "   - These percentiles will provide the lower and upper bounds of the confidence interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1c74af-3140-43c5-9d1c-19fee7161d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for the Population Mean Height: [14.65, 15.91] meters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "np.random.seed(0)  # For reproducibility\n",
    "sample_size = 50\n",
    "mean_height = 15\n",
    "std_dev = 2\n",
    "\n",
    "# Generate original sample data\n",
    "original_sample = np.random.normal(mean_height, std_dev, sample_size)\n",
    "\n",
    "# Bootstrap procedure\n",
    "n_bootstrap = 10000\n",
    "bootstrap_means = np.empty(n_bootstrap)\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Generate a bootstrap sample with replacement\n",
    "    bootstrap_sample = np.random.choice(original_sample, size=sample_size, replace=True)\n",
    "    # Calculate the mean of the bootstrap sample\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Compute the 95% confidence interval\n",
    "lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "print(f\"95% Confidence Interval for the Population Mean Height: [{lower_bound:.2f}, {upper_bound:.2f}] meters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916aa1da-c5f3-439f-8074-1163dc972df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
