{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc98c36-2f55-4d6a-b31f-23b47b69ffb4",
   "metadata": {},
   "source": [
    "## Regression 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edde4c7-202d-4151-8387-14be64938954",
   "metadata": {},
   "source": [
    "#### Q1.  Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df7aa3b-1b9c-4d39-a491-db2fe004d9ac",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Both simple linear and multiple linear regression are Regression algorithms which solve problems that have a continuous dependent/target feature.While simple linear regression finds the best fit line for a linear data using only one independent feature, the multiple linear regression problem also solves linear data problems but it has multiple independent variables that decide the outcome of the target variable.  \n",
    "Simple Linear Regression: $\\hat{y} = b_{0} + b_{1}*x$  \n",
    "Multiple Linear Regression: $\\hat{y} = b_{0} + b_{1}*x_{1} + b_{2}*x_{2} ... b_{n}*x_{n}$  \n",
    "\n",
    "Examples:  \n",
    "Simple Linear Regression:Predicting the Salary of an employee(target variable) using the years of experience as independent variable.   \n",
    "Multiple Linear Regression: Predicting the price of a house(target variable) using multiple features like number of rooms,age of the house,distance from an important location,etc. as independent features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22dd1e-e6c4-4c9e-b39a-ab0d4c0a0dee",
   "metadata": {},
   "source": [
    "#### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\r\n",
    "a given dataset?taset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c51d70a-71f4-4e58-8054-b372c0afd383",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "\n",
    "Linear regression is a widely used statistical method for modeling the relationship between a dependent variable and one or more independent variables. For the results of linear regression to be valid, certain assumptions need to be met. Hereâ€™s a discussion of these assumptions and how to check whether they hold in a given dataset:\n",
    "\n",
    "**Assumptions of Linear Regression**  \n",
    "1. Linearity:\n",
    "  \n",
    "Assumption: The relationship between the dependent variable and each independent variable is linear.  \n",
    "Check: Plot the residuals versus the predicted values. If thereâ€™s a pattern or curvature in the plot, the linearity assumption might be violated.   Alternatively, you can use scatter plots of each predictor against the dependent variable to check for linear relationships.  \n",
    "\n",
    "2. Independence of Errors:\n",
    "  \n",
    "Assumption: The residuals (errors) are independent of each other.  \n",
    "Check: For time series data, plot residuals versus time or use the Durbin-Watson test to check for autocorrelation in residuals. For non-time series data, check if there are patterns in the residuals plot.  \n",
    "\n",
    "3. Homoscedasticity:\n",
    "  \n",
    "Assumption: The variance of the residuals is constant across all levels of the independent variables.  \n",
    "Check: Plot residuals versus the predicted values. If the plot shows a funnel shape or any systematic pattern, it indicates heteroscedasticity. You can also perform formal tests like the Breusch-Pagan test or White test.  \n",
    "\n",
    "4. Normality of Residuals:\n",
    "  \n",
    "Assumption: The residuals (errors) of the model are normally distributed.  \n",
    "Check: Use a Q-Q plot (quantile-quantile plot) to visually inspect if residuals follow a normal distribution. Additionally, you can perform the Shapiro-Wilk test or Kolmogorov-Smirnov test for normality.  \n",
    "\n",
    "5. No Multicollinearity:\n",
    "  \n",
    "Assumption: The independent variables are not highly correlated with each other.  \n",
    "Check: Calculate the Variance Inflation Factor (VIF) for each predictor. A VIF value greater than 10 is often considered indicative of high multicollinearity. Alternatively, you can check the correlation matrix among predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232cd48-3d02-4b44-ad9b-fc9481dbbfe1",
   "metadata": {},
   "source": [
    "#### Q3. How do you interpret slope and intercept in a linear regression model? Provide an example using real world scenario.ario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac27a03-2c4b-49e9-9c36-ee2b9afe769b",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "In a linear regression model, the slope and intercept are crucial for understanding the relationship between the dependent variable and the independent variable(s). Letâ€™s break down their interpretation and provide a real-world example.  \n",
    "\n",
    "**Interpretation of Slope and Intercept**  \n",
    "**Intercept $(ğ›½_{0})$** :\n",
    "      \n",
    "Definition: The intercept is the value of the dependent variable (Y) when all independent variables (X) are equal to zero.  \n",
    "Interpretation: It represents the starting value of Y when X is zero. In practical terms, it often indicates the baseline level of Y in the absence of any influence from X.  \n",
    "\n",
    "**Slope $(ğ›½_{1})$** :\n",
    "  \n",
    "Definition: The slope represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X).  \n",
    "Interpretation: It shows the strength and direction of the relationship between X and Y. A positive slope indicates that as X increases, Y also increases. A negative slope indicates that as X increases, Y decreases.  \n",
    "\n",
    "**Real-World Example: Predicting House Prices**  \n",
    "Letâ€™s use a simple example where we predict house prices based on the size of the house in square feet. Suppose we have a linear regression model:\n",
    "\n",
    "**$Price = ğ›½_{0} + ğ›½_{1} Ã— Size$** \n",
    "  \n",
    "Where:\n",
    "  \n",
    "Price is the house price in dollars.  \n",
    "Size is the size of the house in square feet.  \n",
    "Assume our model produces the following coefficients:  \n",
    "\n",
    "Intercept $(ğ›½_{0})$: 50,000 dollars  \n",
    "Slope $(ğ›½_{1})$: 150 per square foot  \n",
    "  \n",
    "Interpretation  \n",
    "When the size of the house is zero square feet (which is a hypothetical scenario), the predicted price of the house is 50,000 usd. In reality, this might not make practical sense (a house with zero square feet doesn't exist), but it provides a baseline value from which the price changes with size.\n",
    "  \n",
    "For every square feet increase in siz of house the price will increase by 150 usd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa47ffe3-fc17-4dcd-80f0-fb1da6a3de5a",
   "metadata": {},
   "source": [
    "#### Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf34eb-3517-4ead-a737-c23ad4bc489f",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Gradient Descent is an optimization algorithm used to minimize a function by iteratively moving towards the minimum value of the function. It is widely used in machine learning to find the optimal parameters for models, particularly in the context of training algorithms for regression, classification, and neural networks.  \n",
    "\n",
    "How Gradient Descent Works in Machine Learning:  \n",
    "Objective Function:\n",
    "The goal of gradient descent is to minimize a loss or cost function J(Î¸), which measures how well the model performs. For example, in linear regression, this might be the Mean Squared Error (MSE).  \n",
    "\n",
    "Gradient:    \n",
    "The gradient of the loss function with respect to each parameter (Î¸) indicates the direction and rate of the steepest increase of the function. The gradient vector points in the direction of the steepest ascent.  \n",
    "\n",
    "Update Rule:  \n",
    "The core idea is to update the parameters in the opposite direction of the gradient to move towards the minimum. The update rule for the parameters is:\n",
    "\n",
    "$Î¸:=Î¸âˆ’Î±â‹…âˆ‡J(Î¸)$\n",
    "\n",
    "where:  \n",
    "Î¸ represents the parameters of the model.  \n",
    "Î± is the learning rate, which controls the size of the steps taken.  \n",
    "âˆ‡J(Î¸) is the gradient of the loss function with respect to Î¸.  \n",
    "\n",
    "Iteration:  \n",
    "This process is repeated iteratively until convergence, meaning the changes in the parameters become very small or the loss function reaches a plateau.  \n",
    "In machine learning, gradient descent is primarily used to optimize the parameters of a model.  \n",
    "Example: Minimizing Root Mean Square Errror in Linear Regression or Log Loss error in Logistic Regression.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e30ee8-b7ae-46e9-8da7-d788b2841c81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b24ff8e-cd65-4722-8943-ebf8d8ab93d9",
   "metadata": {},
   "source": [
    "#### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37918a83-87b3-48b7-92e9-e2f2fdbe0b21",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Multiple Linear Regression (MLR) is an extension of simple linear regression that models the relationship between a dependent variable and two or more independent variables. The goal is to understand how multiple predictors jointly influence the outcome and to predict the dependent variable based on the values of these predictors.\n",
    "\n",
    "Model Representation\n",
    "The multiple linear regression model can be represented as:\n",
    "\n",
    "$ğ‘¦ = ğ›½0 +ğ›½1ğ‘¥1+ğ›½2ğ‘¥2+â‹¯+ğ›½ğ‘ğ‘¥ğ‘+ğœ–$\n",
    "\n",
    "Where:  \n",
    "y is the dependent variable (target).\n",
    "ğ›½0 is the intercept term.  \n",
    "ğ›½1,ğ›½2,â€¦,ğ›½ğ‘ are the coefficients (slopes) associated with the independent variables ğ‘¥1,ğ‘¥2,â€¦,ğ‘¥ğ‘  \n",
    "Ïµ represents the error term (residuals), capturing the variability in ğ‘¦ that is not explained by the independent variables.  \n",
    "\n",
    "**Differences from Simple Linear Regression**  \n",
    "Simple Linear Regression involves only one independent variable and models the relationship between this single predictor and the dependent variable. The model is represented as:\n",
    "\n",
    "$ğ‘¦ =ğ›½0+ğ›½1ğ‘¥1+ğœ–$  \n",
    "\n",
    "Hereâ€™s how multiple linear regression differs from simple linear regression:  \n",
    "1. Number of Predictors:  \n",
    "Simple Linear Regression: Involves one predictor variable.  \n",
    "Multiple Linear Regression: Involves two or more predictor variables.  \n",
    "\n",
    "2. Model Complexity:  \n",
    "Simple Linear Regression: The model is a straight line (1-dimensional), which makes it easier to visualize and interpret.  \n",
    "Multiple Linear Regression: The model is a hyperplane in a multidimensional space (p-dimensional), which is more complex and harder to visualize.  \n",
    "\n",
    "3. Interpretation:  \n",
    "Simple Linear Regression: The effect of the predictor on the dependent variable is straightforward and unidirectional.  \n",
    "Multiple Linear Regression: Each coefficient represents the effect of a predictor on the dependent variable while holding other predictors constant. This allows for more nuanced insights, but also requires careful interpretation to understand the combined impact of predictors.  \n",
    "\n",
    "4. Interaction Effects:  \n",
    "Simple Linear Regression: Does not account for interactions between predictors.  \n",
    "Multiple Linear Regression: Can include interaction terms (e.g., $ğ‘¥1Ã—ğ‘¥2$) to capture the combined effect of multiple predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2b332-dafd-44e0-9070-cbd30cc2f3b9",
   "metadata": {},
   "source": [
    "#### #### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\r\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dbe62a-2b57-41d3-b6ac-a1b08cedf35e",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Multicollinearity occurs in multiple linear regression when two or more independent variables (predictors) are highly correlated with each other. This means that the independent variables are not providing unique information about the dependent variable; instead, they are overlapping in what they predict.  \n",
    "\n",
    "*Methods to detect multicollinearity:*  \n",
    "1. Correlation Matrix:Compute the pairwise correlation coefficients between the independent variables. High correlation coefficients (close to +1 or -1) suggest potential multicollinearity.\n",
    "2. Variance Inflation Factor (VIF):VIF quantifies how much the variance of a coefficient is inflated due to multicollinearity. A VIF value greater than 10 indicates high multicollinearity.\n",
    "3. Condition Number:The condition number is a measure of multicollinearity based on the eigenvalues of the matrix of independent variables. A condition number above 30 indicates multicollinearity.\n",
    "\n",
    "*Addressing Multicollinearity*:  \n",
    "If multicollinearity is detected, several strategies can be used to address it:  \n",
    "1. Remove Highly Correlated Predictors:Exclude one or more of the highly correlated predictors from the model. This can simplify the model and reduce multicollinearity.  \n",
    "2. Combine Predictors:Create composite variables or principal components to combine the correlated predictors into a single, uncorrelated predictor. Techniques such as Principal Component Analysis (PCA) can help.  \n",
    "3. Regularization Techniques:Use regularization methods like Ridge Regression (L2 regularization) or Lasso Regression (L1 regularization) to penalize large coefficients and reduce multicollinearity. These techniques add a penalty term to the loss function, which can help mitigate the effects of multicollinearity.\n",
    "4. Increase Sample Size:Sometimes, increasing the amount of data can help reduce the variance of the coefficient estimates and mitigate the effects of multicollinearity.  \n",
    "5. Standardize Variables:Standardizing (scaling) the variables can sometimes help reduce multicollinearity, though it is not always effective.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40283ef1-9fb6-4669-a2e0-271c9cc27908",
   "metadata": {},
   "source": [
    "#### Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7c658-2dde-4984-aeac-c701681659f2",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Polynomial Regression is a type of regression analysis that models the relationship between a dependent variable and one or more independent variables using polynomial functions. Unlike linear regression, which fits a straight line to the data, polynomial regression fits a curve, which can better capture complex relationships between variables.  \n",
    "  \n",
    "*Polynomial Regression Representation*  \n",
    "The polynomial regression model extends the simple linear regression model by incorporating polynomial terms. For a single independent variable ğ‘¥, the polynomial regression model is expressed as:\n",
    "  \n",
    "$ğ‘¦=ğ›½0+ğ›½1ğ‘¥+ğ›½2ğ‘¥^2+â‹¯+ğ›½_{ğ‘}ğ‘¥^ğ‘+ğœ–$  \n",
    "\n",
    "Where:  \n",
    "y is the dependent variable (target).\n",
    "ğ›½0,ğ›½1,â€¦,ğ›½ğ‘ are the coefficients of the polynomial terms.  \n",
    "ğ‘¥2,ğ‘¥3,â€¦,ğ‘¥ğ‘ are the polynomial terms of the independent variable ğ‘¥.  \n",
    "Ïµ is the error term.  \n",
    "In the case of multiple independent variables, polynomial regression can include interactions and higher-order terms of these variables, expanding the model to capture more complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba43bf3-a097-4c76-a669-6aaaf850d746",
   "metadata": {},
   "source": [
    "#### Q8. What are the advantages and disadvantages of polynomial regression compared to linear\r\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f81111-c382-4151-a00c-d438ceb44f74",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "**Advantages of Polynomial Regression**\n",
    "1. Flexibility:  \n",
    "Advantage: Polynomial regression can model complex, nonlinear relationships between the dependent and independent variables. It fits curves to the data, allowing for a more accurate representation when the data exhibits a curvilinear trend.  \n",
    "Example: If you have data that follows a parabolic curve, such as the relationship between the amount of fertilizer and crop yield, polynomial regression can fit this curve more effectively than linear regression.  \n",
    "\n",
    "2. Improved Fit:  \n",
    "Advantage: Polynomial regression can provide a better fit to the data by capturing patterns that a straight line might miss. This can result in a lower residual sum of squares (RSS) and a better R-squared value.  \n",
    "Example: In scenarios like predicting the trajectory of a projectile or modeling population growth, polynomial regression can capture the nonlinear trends better.  \n",
    "  \n",
    "3. Higher Predictive Power:  \n",
    "Advantage: By including polynomial terms, the model can achieve higher predictive accuracy in cases where the relationship between variables is nonlinear.  \n",
    "  \n",
    "**Disadvantages of Polynomial Regression**  \n",
    "1. Overfitting:  \n",
    "Disadvantage: Polynomial regression models, especially with high-degree polynomials, are prone to overfitting. They may fit the training data very well but perform poorly on new, unseen data. This happens because the model becomes too complex and captures noise rather than the true underlying relationship.    \n",
    "Example: Using a 10th-degree polynomial for a dataset with only a few data points can lead to an overly complex model that does not generalize well.  \n",
    "\n",
    "2. Increased Complexity:\n",
    "Disadvantage: Higher-degree polynomials can make the model complex and less interpretable. This complexity can make it harder to understand the relationships between variables and the resulting model predictions.  \n",
    "Example: A 5th-degree polynomial can become difficult to interpret compared to a simple linear model, where the relationship is more straightforward.  \n",
    "3. Computational Cost:  \n",
    "Disadvantage: Polynomial regression with high-degree terms can be computationally expensive and time-consuming, especially for large datasets.  \n",
    "4. Extrapolation Issues:  \n",
    "Disadvantage: Polynomial regression can produce unrealistic predictions outside the range of the training data. For instance, high-degree polynomials may oscillate wildly, leading to poor predictions for extrapolated values.  \n",
    "\n",
    "**Situations to Prefer Polynomial Regression**  \n",
    "1. Nonlinear Relationships:  \n",
    "Use polynomial regression when the relationship between the dependent and independent variables is nonlinear and cannot be well represented by a straight line. For example, modeling the growth of a business over time, where growth may accelerate or decelerate in a non-linear fashion.  \n",
    "\n",
    "2. Complex Trends:  \n",
    "When data shows clear curvature or complex patterns, such as seasonal effects or cyclical trends, polynomial regression can provide a more accurate fit.\n",
    "  \n",
    "3. Improving Model Fit:  \n",
    "If initial linear regression results show a poor fit, polynomial regression can be used to improve the model by adding polynomial terms. However, itâ€™s important to validate that the improved fit is not due to overfitting.\n",
    "  \n",
    "4. When Data Suggests Polynomial Fit:  \n",
    "Use polynomial regression if domain knowledge or exploratory data analysis suggests that a polynomial model is appropriate. For instance, modeling the trajectory of an object or fitting a polynomial curve to smooth noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff41210-babf-4586-88b6-d535f26fa2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
