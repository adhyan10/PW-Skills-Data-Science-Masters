{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bdb797c-7b97-4511-86ec-afacf6f7ce97",
   "metadata": {},
   "source": [
    "# Naive Bayes 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bcf6da-8a5d-4d36-bc13-db19e059f616",
   "metadata": {},
   "source": [
    "**Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193cc810-eeaf-4161-a0f9-bfe721503709",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "\n",
    "**Determine the Probability \\( P(S \\mid I) \\)**\n",
    "\n",
    "To find \\( P(S \\mid I) \\) when \\( P(I \\mid S) = 0.40 \\), we use Bayes' theorem. \n",
    "\n",
    "**Define the probabilities:**\n",
    "- \\( P(I) \\) = Probability that an employee uses the health insurance plan = 0.70.\n",
    "- \\( P(I \\mid S) \\) = Probability that an employee uses the health insurance plan given that they are a smoker = 0.40.\n",
    "\n",
    "**Bayes' Theorem:**\n",
    "\n",
    "Bayes' theorem states:\n",
    "\n",
    "$$\n",
    "P(S \\mid I) = \\frac{P(I \\mid S) \\cdot P(S)}{P(I)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( P(S \\mid I) \\) is the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "- \\( P(I \\mid S) \\) is the probability that an employee uses the health insurance plan given that they are a smoker.\n",
    "- \\( P(S) \\) is the overall probability that an employee is a smoker.\n",
    "- \\( P(I) \\) is the overall probability that an employee uses the health insurance plan.\n",
    "\n",
    "**Find \\( P(S) \\) using the Law of Total Probability:**\n",
    "\n",
    "The total probability of using the insurance plan can be found by considering both smokers and non-smokers:\n",
    "\n",
    "$$\n",
    "P(I) = P(I \\mid S) \\cdot P(S) + P(I \\mid S') \\cdot P(S')\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $P(S')$ = Probability of not being a smoker.\n",
    "- $P(I \\mid S')$ = Probability that an employee uses the health insurance plan given that they are not a smoker.\n",
    "\n",
    "Rearranging to solve for \\( P(S) \\):\n",
    "\n",
    "$$\n",
    "P(S) = \\frac{P(I) - P(I \\mid S') \\cdot P(S')}{P(I \\mid S)}\n",
    "$$\n",
    "\n",
    "**Determine $P(I \\mid S')$ and $P(S')$:**\n",
    "\n",
    "- $P(S') = 1 - P(S)$.\n",
    "\n",
    "If we assume we don’t have additional information about non-smokers, let’s estimate $P(S)3$:\n",
    "\n",
    "Rearranging the equations with estimated values:\n",
    "\n",
    "$$\n",
    "P(S) \\approx \\frac{P(I) - P(I \\mid S') \\cdot (1 - P(S))}{P(I \\mid S)}\n",
    "$$\n",
    "\n",
    "Given no specific $P(I \\mid S')$, using simplified values provides a reasonable estimate.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Given  $P(I \\mid S) = 0.40$, the probability $P(S \\mid I)$ is approximately:\n",
    "\n",
    "$$\n",
    "P(S \\mid I) \\approx 0.40\n",
    "$$\n",
    "\n",
    "Hence, $P(S \\mid I) \\approx \\boxed{0.40}$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ed023-d236-4820-b244-15ef57c9999c",
   "metadata": {},
   "source": [
    "**Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc094d0f-d654-4556-af34-a7172dd4928f",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "\n",
    "**Difference Between Bernoulli Naive Bayes and Multinomial Naive Bayes**\n",
    "\n",
    "**1. Data Type:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:**\n",
    "  - Suitable for binary/boolean features.\n",
    "  - Assumes that each feature is binary (e.g., 0 or 1, true or false).\n",
    "  - Works well for text classification problems where the presence or absence of words is more important than their frequency. For example, in spam detection, it can work with whether or not a specific word appears in an email.\n",
    "\n",
    "- **Multinomial Naive Bayes:**\n",
    "  - Suitable for categorical features where the feature values are counts or frequencies.\n",
    "  - Assumes that features are counts or frequencies of events.\n",
    "  - Works well for text classification problems where the frequency of word occurrence matters, such as document classification, where the number of times a word appears is important.\n",
    "\n",
    "**2. Probability Model:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:**\n",
    "  - Models the probability of a feature being present or absent.\n",
    "  - The feature is modeled as a Bernoulli random variable, i.e., it takes on values 0 or 1.\n",
    "  - The likelihood of a feature given a class is modeled as a Bernoulli distribution:\n",
    "    $$\n",
    "    P(x_i \\mid y) = \\text{Bernoulli}(p_i)\n",
    "    $$\n",
    "    where $p_i$ is the probability of feature $x_i$ being present for class y.\n",
    "\n",
    "- **Multinomial Naive Bayes:**\n",
    "  - Models the probability of feature counts.\n",
    "  - The feature is modeled as a multinomial random variable, which counts occurrences of features.\n",
    "  - The likelihood of a feature given a class is modeled as a Multinomial distribution:\n",
    "    $$\n",
    "    P(x_i \\mid y) = \\frac{(N_i)!}{x_i! \\cdot (N_i - x_i)!} \\cdot p_i^{x_i} \\cdot (1 - p_i)^{N_i - x_i}\n",
    "    $$\n",
    "    where $x_i$ is the count of feature i, $N_i$ is the total number of features in class y, and $p_i$ is the probability of feature i given class y.\n",
    "\n",
    "**3. Use Case Examples:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:**\n",
    "  - Text classification where the presence/absence of words is important, but not their frequency. Example: spam email classification where the presence of specific keywords determines spam classification.\n",
    "\n",
    "- **Multinomial Naive Bayes:**\n",
    "  - Text classification where word frequencies matter. Example: sentiment analysis where the number of times certain words appear can be indicative of sentiment.\n",
    "\n",
    "**4. Feature Representation:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:**\n",
    "  - Features are binary, typically represented as a binary vector indicating the presence (1) or absence (0) of a word or feature.\n",
    "\n",
    "- **Multinomial Naive Bayes:**\n",
    "  - Features are counts or frequencies, represented as vectors where each value corresponds to the count of occurrences of a word or feature in a document.\n",
    "\n",
    "**5. Mathematical Formulation:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:**\n",
    "  - For a binary feature vector \\( x \\) and class \\( y \\):\n",
    "    $$\n",
    "    P(y \\mid x) \\propto P(y) \\prod_{i=1}^n P(x_i \\mid y)\n",
    "    $$\n",
    "    where $x_i$ is 0 or 1, and  $P(x_i \\mid y)$ is the probability of feature i being present in class y.\n",
    "\n",
    "- **Multinomial Naive Bayes:**\n",
    "  - For a feature vector \\( x \\) with counts and class \\( y \\):\n",
    "    $$\n",
    "    P(y \\mid x) \\propto P(y) \\prod_{i=1}^n \\frac{(N_i)!}{x_i! \\cdot (N_i - x_i)!} \\cdot p_i^{x_i} \\cdot (1 - p_i)^{N_i - x_i}\n",
    "    $$\n",
    "    where $x_i$ is the count of feature i, and $N_i$  is the total number of occurrences.\n",
    "\n",
    "In summary, the key difference lies in the nature of the features they handle: Bernoulli Naive Bayes works with binary features indicating presence or absence, while Multinomial Naive Bayes handles features represented as counts or frequencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e8087-3756-420a-8349-ad1afd39fbd8",
   "metadata": {},
   "source": [
    "**Q3. How does Bernoulli Naive Bayes handle missing values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9df6a-3f79-4bd8-b2d8-787bc3fc95e6",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "\n",
    "**Handling Missing Values in Bernoulli Naive Bayes**\n",
    "\n",
    "Handling missing values is an important aspect of data preprocessing in machine learning. Bernoulli Naive Bayes, specifically designed for binary features, requires careful handling of missing values due to its binary nature. Here’s how it can handle missing values:\n",
    "\n",
    "**1. Ignoring Missing Values:**\n",
    "- **Default Approach:** One straightforward approach is to ignore missing values during the training and prediction processes. This means if a feature value is missing for a particular instance, it is simply not used in the computation of probabilities. The Naive Bayes classifier will use only the features with available values.\n",
    "- **Impact:** This approach is generally effective if the proportion of missing values is small. However, if missing values are common, this can lead to a loss of valuable information and potentially biased results.\n",
    "\n",
    "**2. Imputation:**\n",
    "- **Replacing Missing Values:** Another approach is to impute the missing values before applying the Bernoulli Naive Bayes algorithm. Imputation can be done using several methods:\n",
    "  - **Mode Imputation:** For binary features, you can replace missing values with the most common value (mode) of the feature in the training data.\n",
    "  - **Random Imputation:** Replace missing values with randomly sampled values from the feature’s distribution.\n",
    "  - **Model-Based Imputation:** Use another model to predict the missing values based on other available features.\n",
    "- **Impact:** Imputation can help in making full use of available data but may introduce noise if not done carefully.\n",
    "\n",
    "**3. Modifying the Algorithm:**\n",
    "- **Augmented Bernoulli Naive Bayes:** Some variations of Naive Bayes can be adapted to handle missing values directly. For example, you could modify the Bernoulli Naive Bayes algorithm to include a special category for missing values, although this is less common.\n",
    "- **Impact:** This approach may require custom implementation and may not be as straightforward as the standard Bernoulli Naive Bayes approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0653b10-4c89-4ad6-99a4-8b8363b770a9",
   "metadata": {},
   "source": [
    "**Q4. Can Gaussian Naive Bayes be used for multi-class classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a67fb2d-12ed-465c-a73c-5e4acfec2d72",
   "metadata": {},
   "source": [
    "**Ans:**  \n",
    "  \n",
    "Yes, Gaussian Naive Bayes can indeed be used for multi-class classification. The Gaussian Naive Bayes classifier is well-suited for problems where the features are continuous and are assumed to follow a Gaussian (normal) distribution. It can handle multiple classes effectively. Here’s how it works for multi-class classification:\n",
    "\n",
    "**1. Gaussian Naive Bayes Basics:**\n",
    "- **Assumption:** Each feature is assumed to be normally distributed within each class.\n",
    "- **Probability Density Function:** For a continuous feature $x_i$ in class y, the probability density function of the Gaussian distribution is:\n",
    "  $$\n",
    "  P(x_i \\mid y) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x_i - \\mu)^2}{2 \\sigma^2} \\right)\n",
    "  $$\n",
    "  where $\\mu$ is the mean and $\\sigma^2$  is the variance of the feature in class y.\n",
    "\n",
    "**2. Multi-Class Classification:**\n",
    "- **Class Probabilities:** Gaussian Naive Bayes calculates the posterior probability for each class using Bayes' theorem. Given a feature vector  $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ , the classifier computes:\n",
    "  $$\n",
    "  P(y \\mid \\mathbf{x}) \\propto P(y) \\prod_{i=1}^n P(x_i \\mid y)\n",
    "  $$\n",
    "  where $P(y)$  is the prior probability of class y, and $P(x_i \\mid y)$ is the likelihood of feature $x_i$ given class $y$, modeled as a Gaussian distribution.\n",
    "  \n",
    "- **Classification Decision:** The class with the highest posterior probability is chosen as the predicted class:\n",
    "  $$\n",
    "  \\hat{y} = \\arg \\max_y P(y \\mid \\mathbf{x})\n",
    "  $$\n",
    "\n",
    "**3. Handling Multiple Classes:**\n",
    "- **Training:** During training, the model estimates the parameters $\\mu$ and $\\sigma^2$ for each feature and each class. This involves calculating the mean and variance of each feature for each class from the training data.\n",
    "- **Prediction:** During prediction, the model evaluates the posterior probability for each class based on the Gaussian distribution of each feature and selects the class with the highest probability.\n",
    "\n",
    "**4. Advantages for Multi-Class Classification:**\n",
    "- **Scalability:** Gaussian Naive Bayes is computationally efficient and scales well with the number of features and classes.\n",
    "- **Simplicity:** The model is simple and easy to interpret, making it a good choice for many practical applications.\n",
    "\n",
    "This makes Gaussian Naive Bayes a versatile tool in machine learning, capable of handling multiple classes effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e024525-7f64-46ac-9f3a-bf6d05e3fd0d",
   "metadata": {},
   "source": [
    "**Q5. Assignment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec125f7-9f41-461b-bc0d-1693827de194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB,GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009fb264-cddf-4d74-be5c-3212da329bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"spambase.data\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff41e05-cab8-4673-ba9e-05718bc599bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '0.64', '0.64.1', '0.1', '0.32', '0.2', '0.3', '0.4', '0.5', '0.6',\n",
       "       '0.7', '0.64.2', '0.8', '0.9', '0.10', '0.32.1', '0.11', '1.29', '1.93',\n",
       "       '0.12', '0.96', '0.13', '0.14', '0.15', '0.16', '0.17', '0.18', '0.19',\n",
       "       '0.20', '0.21', '0.22', '0.23', '0.24', '0.25', '0.26', '0.27', '0.28',\n",
       "       '0.29', '0.30', '0.31', '0.33', '0.34', '0.35', '0.36', '0.37', '0.38',\n",
       "       '0.39', '0.40', '0.41', '0.42', '0.43', '0.778', '0.44', '0.45',\n",
       "       '3.756', '61', '278', '1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb3b6a8-cf93-4cdd-9237-ebf6aebd953e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.6</th>\n",
       "      <th>...</th>\n",
       "      <th>0.41</th>\n",
       "      <th>0.42</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.44</th>\n",
       "      <th>0.45</th>\n",
       "      <th>3.756</th>\n",
       "      <th>61</th>\n",
       "      <th>278</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  0.64  0.64.1  0.1  0.32   0.2   0.3   0.4   0.5   0.6  ...  0.41  \\\n",
       "0  0.21  0.28    0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "1  0.06  0.00    0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "2  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "3  0.00  0.00    0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00    0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...  0.00   \n",
       "\n",
       "    0.42  0.43  0.778   0.44   0.45  3.756   61   278  1  \n",
       "0  0.132   0.0  0.372  0.180  0.048  5.114  101  1028  1  \n",
       "1  0.143   0.0  0.276  0.184  0.010  9.821  485  2259  1  \n",
       "2  0.137   0.0  0.137  0.000  0.000  3.537   40   191  1  \n",
       "3  0.135   0.0  0.135  0.000  0.000  3.537   40   191  1  \n",
       "4  0.223   0.0  0.000  0.000  0.000  3.000   15    54  1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0265512-a6de-4cf2-8a22-a5a26d096342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4600, 58)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b231934a-71c5-49ae-ac71-e3c05ac93f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"1\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a13f005b-ad34-41d9-8131-78622c1114c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\n",
       "0    2788\n",
       "1    1812\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad952ed-5b48-441b-9620-19c22ae13aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"1\",axis = 1)\n",
    "y = data[\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d5a4c6-f0ae-40fb-829a-833374c5730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X:(4600, 57)\n",
      "shape of Y:(4600,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape of X:{X.shape}\")\n",
    "print(f\"shape of Y:{y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826601f2-5d5e-47a7-af5c-4124c6845bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'BernoulliNB' : BernoulliNB(),\n",
    "            'MultinomialNB' : MultinomialNB(),\n",
    "            'GaussianNB' : GaussianNB()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b200cd83-46b5-4338-90d8-b5885ac66821",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72bb7b28-5b86-47d4-9781-5a7a17186bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,model in models.items():\n",
    "    if name == \"MultinomialNB\":\n",
    "        scaler = MinMaxScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        cv_results = cross_validate(model,X_scaled,y,cv=10,\n",
    "                                   scoring=['accuracy','precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "        results[name] = {\n",
    "            'accuracy': cv_results['test_accuracy'],\n",
    "            'precision': cv_results['test_precision_weighted'],\n",
    "            'recall': cv_results['test_recall_weighted'],\n",
    "            'f1': cv_results['test_f1_weighted'],\n",
    "            'mean_accuracy': cv_results['test_accuracy'].mean(),\n",
    "            'mean_precision': cv_results['test_precision_weighted'].mean(),\n",
    "            'mean_recall': cv_results['test_recall_weighted'].mean(),\n",
    "            'mean_f1': cv_results['test_f1_weighted'].mean(),\n",
    "            'std_accuracy': cv_results['test_accuracy'].std(),\n",
    "            'std_precision': cv_results['test_precision_weighted'].std(),\n",
    "            'std_recall': cv_results['test_recall_weighted'].std(),\n",
    "            'std_f1': cv_results['test_f1_weighted'].std(),\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        cv_results = cross_validate(model,X_scaled,y,cv=10,\n",
    "                                   scoring=['accuracy','precision_weighted', 'recall_weighted', 'f1_weighted'])\n",
    "\n",
    "        results[name] = {\n",
    "            'accuracy': cv_results['test_accuracy'],\n",
    "            'precision': cv_results['test_precision_weighted'],\n",
    "            'recall': cv_results['test_recall_weighted'],\n",
    "            'f1': cv_results['test_f1_weighted'],\n",
    "            'mean_accuracy': cv_results['test_accuracy'].mean(),\n",
    "            'mean_precision': cv_results['test_precision_weighted'].mean(),\n",
    "            'mean_recall': cv_results['test_recall_weighted'].mean(),\n",
    "            'mean_f1': cv_results['test_f1_weighted'].mean(),\n",
    "            'std_accuracy': cv_results['test_accuracy'].std(),\n",
    "            'std_precision': cv_results['test_precision_weighted'].std(),\n",
    "            'std_recall': cv_results['test_recall_weighted'].std(),\n",
    "            'std_f1': cv_results['test_f1_weighted'].std(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c46d5887-0f56-478b-b960-da7e7107c94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BernoulliNB\n",
      "Cross-validation accuracy scores: [0.90217391 0.92391304 0.90217391 0.91956522 0.90434783 0.92608696\n",
      " 0.95       0.91304348 0.84347826 0.83043478]\n",
      "Mean accuracy: 0.90\n",
      "Standard deviation of accuracy: 0.04\n",
      "Cross-validation precision scores: [0.90569379 0.92440437 0.90377174 0.92028816 0.90462648 0.92605263\n",
      " 0.95132161 0.91589505 0.8462241  0.82937495]\n",
      "Mean precision: 0.90\n",
      "Standard deviation of precision: 0.04\n",
      "Cross-validation recall scores: [0.90217391 0.92391304 0.90217391 0.91956522 0.90434783 0.92608696\n",
      " 0.95       0.91304348 0.84347826 0.83043478]\n",
      "Mean recall: 0.90\n",
      "Standard deviation of recall: 0.04\n",
      "Cross-validation F1 scores: [0.90052033 0.92337321 0.90093572 0.91887952 0.90359436 0.92577413\n",
      " 0.94957376 0.91174918 0.84426929 0.82931285]\n",
      "Mean F1 score: 0.90\n",
      "Standard deviation of F1 score: 0.04\n",
      "\n",
      "** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **\n",
      "\n",
      "Model: MultinomialNB\n",
      "Cross-validation accuracy scores: [0.85434783 0.89565217 0.85652174 0.91086957 0.82826087 0.90434783\n",
      " 0.93043478 0.87826087 0.82826087 0.8673913 ]\n",
      "Mean accuracy: 0.88\n",
      "Standard deviation of accuracy: 0.03\n",
      "Cross-validation precision scores: [0.87060141 0.89925857 0.86295179 0.91331948 0.82881616 0.90421762\n",
      " 0.93558727 0.8853097  0.82740037 0.87224757]\n",
      "Mean precision: 0.88\n",
      "Standard deviation of precision: 0.03\n",
      "Cross-validation recall scores: [0.85434783 0.89565217 0.85652174 0.91086957 0.82826087 0.90434783\n",
      " 0.93043478 0.87826087 0.82826087 0.8673913 ]\n",
      "Mean recall: 0.88\n",
      "Standard deviation of recall: 0.03\n",
      "Cross-validation F1 scores: [0.84794079 0.89380584 0.85240865 0.90961017 0.82850249 0.90383114\n",
      " 0.92918084 0.87500279 0.82763154 0.86420792]\n",
      "Mean F1 score: 0.87\n",
      "Standard deviation of F1 score: 0.03\n",
      "\n",
      "** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **\n",
      "\n",
      "Model: GaussianNB\n",
      "Cross-validation accuracy scores: [0.84347826 0.86521739 0.87826087 0.85869565 0.88478261 0.82608696\n",
      " 0.82391304 0.86086957 0.6326087  0.71304348]\n",
      "Mean accuracy: 0.82\n",
      "Standard deviation of accuracy: 0.08\n",
      "Cross-validation precision scores: [0.87361495 0.89197152 0.90014868 0.8900807  0.89747532 0.874603\n",
      " 0.8667521  0.87696391 0.7637595  0.77144232]\n",
      "Mean precision: 0.86\n",
      "Standard deviation of precision: 0.05\n",
      "Cross-validation recall scores: [0.84347826 0.86521739 0.87826087 0.85869565 0.88478261 0.82608696\n",
      " 0.82391304 0.86086957 0.6326087  0.71304348]\n",
      "Mean recall: 0.82\n",
      "Standard deviation of recall: 0.08\n",
      "Cross-validation F1 scores: [0.84511977 0.86667418 0.87965816 0.86024336 0.88600619 0.82746432\n",
      " 0.82551383 0.86241965 0.61925071 0.71402019]\n",
      "Mean F1 score: 0.82\n",
      "Standard deviation of F1 score: 0.08\n",
      "\n",
      "** **** **** **** **** **** **** **** **** **** **** **** **** **** **** **\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output the results\n",
    "for name, result in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Cross-validation accuracy scores: {result['accuracy']}\")\n",
    "    print(f\"Mean accuracy: {result['mean_accuracy']:.2f}\")\n",
    "    print(f\"Standard deviation of accuracy: {result['std_accuracy']:.2f}\")\n",
    "    print(f\"Cross-validation precision scores: {result['precision']}\")\n",
    "    print(f\"Mean precision: {result['mean_precision']:.2f}\")\n",
    "    print(f\"Standard deviation of precision: {result['std_precision']:.2f}\")\n",
    "    print(f\"Cross-validation recall scores: {result['recall']}\")\n",
    "    print(f\"Mean recall: {result['mean_recall']:.2f}\")\n",
    "    print(f\"Standard deviation of recall: {result['std_recall']:.2f}\")\n",
    "    print(f\"Cross-validation F1 scores: {result['f1']}\")\n",
    "    print(f\"Mean F1 score: {result['mean_f1']:.2f}\")\n",
    "    print(f\"Standard deviation of F1 score: {result['std_f1']:.2f}\")\n",
    "    print()\n",
    "    print(\"** **\"*15)  # Newline for better readability\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8dbe1-3540-4551-9787-31d8f7b964ce",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "1. Bernoulli Naive Bayes Performed better in comaprison to Multinomial and Gaussion NAive Bayes base models in all metrics like accuracy score, precision, recall and F1 score.\n",
    "2. According to the status quo, either multinomial or gaussian was meant to work better than bernuolli naive bayes,as MultiNomial works better on frequencies of words and Gaussian works better with continuous data, but I guess since we did not use any hyperparameters or its tuning, they underperformed, as they are more complex in nature ,hyperparmeter tuning is essential for it to work   effectively.\n",
    "3. Following are the limitations of Naive Bayes Algorithms:\n",
    "   - Assumption of feature independence.\n",
    "   - Poor performance with correlated features.\n",
    "   - Zero probability problem.\n",
    "    - Assumption of normality in Gaussian Naive Bayes.\n",
    "    - Difficulty in capturing complex relationships.\n",
    "    - Sensitivity to feature representation.\n",
    "    - Challenges with large feature spaces.\n",
    "d to consider alternative models if necessary.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad994bcb-cf60-4cb5-a3a5-2f9a6bd1b434",
   "metadata": {},
   "source": [
    "**Conclusion**:  \n",
    "\n",
    "1. For the current situation Bernuolli Naive Bayes will be used as the Final Model.\n",
    "2. But if usage of other parameters is allowed in future we can find the best parameters with hyperparameter tuning techniques like GridSearchCV and RandomSearchCV.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5ec74-1bd7-4d06-ba3b-71fd46b4ece0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
